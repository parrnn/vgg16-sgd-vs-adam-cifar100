{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **VGG16 Training on CIFAR-100 (SGD Optimizer)**"
      ],
      "metadata": {
        "id": "DL539rJRctBV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3mWO0XMRLxgi"
      },
      "outputs": [],
      "source": [
        "# Imports libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Device Configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WzpiSuvbMEfu"
      },
      "outputs": [],
      "source": [
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Dataset Preparation**\n",
        "Load CIFAR-100 and apply standard preprocessing\n"
      ],
      "metadata": {
        "id": "MAAbbkbcc4Nc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PqnF5ezKMGkI"
      },
      "outputs": [],
      "source": [
        "# Convert input images to tensors and normalize values\n",
        "transform = transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor(),transforms.transforms.Normalize(\n",
        "    mean=(0.5071, 0.4867, 0.4408),\n",
        "    std=(0.2675, 0.2565, 0.2761)\n",
        ")\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR100(root=\"./data\", train=True,download=True, transform=transform, )\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = datasets.CIFAR100(root=\"./data\", train=False,download=True, transform=transform, )\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Architecture**\n",
        "VGG16 architecture with Batch Normalization to accelerate convergence"
      ],
      "metadata": {
        "id": "iAdJX4PtdNdU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MqnZFeK0Ms1I"
      },
      "outputs": [],
      "source": [
        "class VGG16(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super().__init__()\n",
        "        # Block 1\n",
        "        self.C1  = nn.Conv2d(3,   64, 3, 1, 1);  self.B1  = nn.BatchNorm2d(64)\n",
        "        self.C2  = nn.Conv2d(64,  64, 3, 1, 1);  self.B2  = nn.BatchNorm2d(64)\n",
        "        self.S3  = nn.MaxPool2d(2,2)\n",
        "\n",
        "        # Block 2\n",
        "        self.C4  = nn.Conv2d(64,  128, 3, 1, 1); self.B4  = nn.BatchNorm2d(128)\n",
        "        self.C5  = nn.Conv2d(128, 128, 3, 1, 1); self.B5  = nn.BatchNorm2d(128)\n",
        "        self.S6  = nn.MaxPool2d(2,2)\n",
        "\n",
        "        # Block 3\n",
        "        self.C7  = nn.Conv2d(128, 256, 3, 1, 1); self.B7  = nn.BatchNorm2d(256)\n",
        "        self.C8  = nn.Conv2d(256, 256, 3, 1, 1); self.B8  = nn.BatchNorm2d(256)\n",
        "        self.C9  = nn.Conv2d(256, 256, 3, 1, 1); self.B9  = nn.BatchNorm2d(256)\n",
        "        self.S10 = nn.MaxPool2d(2,2)\n",
        "\n",
        "        # Block 4\n",
        "        self.C11 = nn.Conv2d(256, 512, 3, 1, 1); self.B11 = nn.BatchNorm2d(512)\n",
        "        self.C12 = nn.Conv2d(512, 512, 3, 1, 1); self.B12 = nn.BatchNorm2d(512)\n",
        "        self.C13 = nn.Conv2d(512, 512, 3, 1, 1); self.B13 = nn.BatchNorm2d(512)\n",
        "        self.S14 = nn.MaxPool2d(2,2)\n",
        "\n",
        "        # Block 5\n",
        "        self.C15 = nn.Conv2d(512, 512, 3, 1, 1); self.B15 = nn.BatchNorm2d(512)\n",
        "        self.C16 = nn.Conv2d(512, 512, 3, 1, 1); self.B16 = nn.BatchNorm2d(512)\n",
        "        self.C17 = nn.Conv2d(512, 512, 3, 1, 1); self.B17 = nn.BatchNorm2d(512)\n",
        "        self.S18 = nn.MaxPool2d(2,2)\n",
        "\n",
        "        # Head\n",
        "        self.pool7 = nn.AdaptiveAvgPool2d((7,7))\n",
        "        self.drop  = nn.Dropout(0.5)\n",
        "        self.F1    = nn.Linear(512*7*7, 4096)\n",
        "        self.F2    = nn.Linear(4096, 4096)\n",
        "        self.F3    = nn.Linear(4096, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = F.relu(self.B1(self.C1(x)))\n",
        "        x = F.relu(self.B2(self.C2(x)))\n",
        "        x = self.S3(x)\n",
        "\n",
        "        # Block 2\n",
        "        x = F.relu(self.B4(self.C4(x)))\n",
        "        x = F.relu(self.B5(self.C5(x)))\n",
        "        x = self.S6(x)\n",
        "\n",
        "        # Block 3\n",
        "        x = F.relu(self.B7(self.C7(x)))\n",
        "        x = F.relu(self.B8(self.C8(x)))\n",
        "        x = F.relu(self.B9(self.C9(x)))\n",
        "        x = self.S10(x)\n",
        "\n",
        "        # Block 4\n",
        "        x = F.relu(self.B11(self.C11(x)))\n",
        "        x = F.relu(self.B12(self.C12(x)))\n",
        "        x = F.relu(self.B13(self.C13(x)))\n",
        "        x = self.S14(x)\n",
        "\n",
        "        # Block 5\n",
        "        x = F.relu(self.B15(self.C15(x)))\n",
        "        x = F.relu(self.B16(self.C16(x)))\n",
        "        x = F.relu(self.B17(self.C17(x)))\n",
        "        x = self.S18(x)\n",
        "\n",
        "        # Head\n",
        "        x = self.pool7(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.drop(F.relu(self.F1(x)))\n",
        "        x = self.drop(F.relu(self.F2(x)))\n",
        "        x = self.F3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training Setup**\n",
        "\n",
        "The model is trained on the CIFAR-100 dataset using a VGG16 architecture with 100 output classes.  \n",
        "**Stochastic Gradient Descent (SGD)** with momentum is used as the optimizer to provide stable updates and improved convergence behavior.\n",
        "\n",
        "A higher learning rate is selected compared to adaptive optimizers, while momentum helps smooth parameter updates and accelerate training.\n"
      ],
      "metadata": {
        "id": "FqSMvpVhdxjU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1EgJN-tQMxiQ"
      },
      "outputs": [],
      "source": [
        "num_classes = 100\n",
        "num_epochs = 5\n",
        "learning_rate = 0.005\n",
        "model = VGG16(num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate,\n",
        "    momentum=0.9\n",
        ")\n",
        "total_step = len(train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training Loop**\n",
        "\n",
        "The model is trained epoch by epoch and evaluated on the test set after each epoch.\n"
      ],
      "metadata": {
        "id": "QTw3DpAad6TM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZjQZ-XU4M90G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ef6d5d2-853a-45b9-d96d-304e6e9a851a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5]\n",
            "Train Loss: 4.0017 | Train Acc: 7.87%\n",
            "Test  Loss: 3.5219 | Test  Acc: 16.68%\n",
            "Epoch [2/5]\n",
            "Train Loss: 3.3512 | Train Acc: 18.46%\n",
            "Test  Loss: 2.9820 | Test  Acc: 26.11%\n",
            "Epoch [3/5]\n",
            "Train Loss: 2.9127 | Train Acc: 26.77%\n",
            "Test  Loss: 2.6463 | Test  Acc: 32.38%\n",
            "Epoch [4/5]\n",
            "Train Loss: 2.5359 | Train Acc: 34.21%\n",
            "Test  Loss: 2.4335 | Test  Acc: 37.32%\n",
            "Epoch [5/5]\n",
            "Train Loss: 2.2326 | Train Acc: 40.28%\n",
            "Test  Loss: 2.1116 | Test  Acc: 43.60%\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    # Train\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    train_acc = 100.0 * correct_train / total_train\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # Test\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "            total_test += labels.size(0)\n",
        "\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "    test_acc = 100.0 * correct_test / total_test\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Test  Loss: {avg_test_loss:.4f} | Test  Acc: {test_acc:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}